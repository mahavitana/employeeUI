{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1c0KisReTXJKlHK_urDMGY-j3333XLlJq",
      "authorship_tag": "ABX9TyM6XsXI1zrCxCOuZ+X3cvzz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eme4Mpol7KME",
        "outputId": "a1ae5001-94ae-4ed8-f7ae-123d54aab750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (43.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy scikit-learn matplotlib seaborn cryptography"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SIT326 - Pass Task 4: Machine Learning for Malicious Traffic Detection\n",
        "\n",
        "# Import necessary libraries\n",
        "import socket\n",
        "import ssl\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from cryptography import x509\n",
        "from cryptography.x509.oid import NameOID, ExtensionOID\n",
        "from cryptography.hazmat.primitives import hashes\n",
        "import os\n",
        "import logging\n",
        "from tqdm import tqdm # For progress bars\n",
        "\n",
        "# --- Machine Learning Imports ---\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import tree\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Configuration ---\n",
        "BENIGN_HOSTS_FILE = '/content/drive/MyDrive/SIT326P4/benign_hosts.txt'\n",
        "MALICIOUS_HOSTS_FILE = '/content/drive/MyDrive/SIT326P4/malicious_hosts.txt'\n",
        "OUTPUT_CSV_FILE = '/content/drive/MyDrive/SIT326P4/tls_certificates_data.csv'\n",
        "DECISION_TREE_IMG = '/content/drive/MyDrive/SIT326P4/decision_tree.png'\n",
        "FEATURE_IMPORTANCE_IMG = '/content/drive/MyDrive/SIT326P4/feature_importance.png'\n",
        "\n",
        "# Setup basic logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 1: DATA COLLECTION & FEATURE EXTRACTION\n",
        "# ==============================================================================\n",
        "\n",
        "def get_certificate_details(hostname, port=443):\n",
        "    \"\"\"\n",
        "    Connects to a host and retrieves its TLS certificate details.\n",
        "    Returns a dictionary of features or None if connection fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a default SSL context\n",
        "        context = ssl.create_default_context()\n",
        "        # Connect to the server\n",
        "        with socket.create_connection((hostname, port), timeout=5) as sock:\n",
        "            with context.wrap_socket(sock, server_hostname=hostname) as ssock:\n",
        "                # Get the certificate in binary DER format\n",
        "                cert_der = ssock.getpeercert(True)\n",
        "                # Parse the certificate using the cryptography library\n",
        "                cert = x509.load_der_x509_certificate(cert_der)\n",
        "\n",
        "                # --- Feature Extraction ---\n",
        "                features = {}\n",
        "\n",
        "                # 1. Validity Period (in days)\n",
        "                validity_period = (cert.not_valid_after - cert.not_valid_before).days\n",
        "                features['validity_period'] = validity_period\n",
        "\n",
        "                # 2. Public Key Length (in bits)\n",
        "                key_length = cert.public_key().key_size\n",
        "                features['key_length'] = key_length\n",
        "\n",
        "                # 3. Signature Algorithm\n",
        "                sig_algo = cert.signature_hash_algorithm.name\n",
        "                features['signature_algorithm'] = sig_algo.upper()\n",
        "\n",
        "                # 4. Issuer Organization\n",
        "                try:\n",
        "                    issuer_org = cert.issuer.get_attributes_for_oid(NameOID.ORGANIZATION_NAME)[0].value\n",
        "                except IndexError:\n",
        "                    issuer_org = 'N/A' # Handle cases where Org is not present\n",
        "                features['issuer_org'] = issuer_org\n",
        "\n",
        "                # 5. Subject Alternative Name (SAN) Count\n",
        "                try:\n",
        "                    san_extension = cert.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n",
        "                    san_count = len(san_extension.value.get_values_for_type(x509.DNSName))\n",
        "                except x509.ExtensionNotFound:\n",
        "                    san_count = 0\n",
        "                features['san_count'] = san_count\n",
        "\n",
        "                # 6. Self-Signed Status (1 if self-signed, 0 otherwise)\n",
        "                is_self_signed = 1 if cert.issuer == cert.subject else 0\n",
        "                features['self_signed'] = is_self_signed\n",
        "\n",
        "                return features\n",
        "\n",
        "    except (socket.gaierror, socket.timeout, ConnectionRefusedError, ssl.SSLError, ssl.CertificateError, OSError) as e:\n",
        "        # logging.warning(f\"Could not connect to or get cert for {hostname}: {e}\")\n",
        "        return None\n",
        "\n",
        "def collect_data(host_file, label):\n",
        "    \"\"\"\n",
        "    Reads a file of hostnames, collects certificate data for each,\n",
        "    and returns a list of dictionaries.\n",
        "    \"\"\"\n",
        "    certificates_data = []\n",
        "    with open(host_file, 'r') as f:\n",
        "        hosts = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    # Use tqdm for a nice progress bar\n",
        "    for host in tqdm(hosts, desc=f\"Processing {label} hosts\"):\n",
        "        details = get_certificate_details(host)\n",
        "        if details:\n",
        "            details['label'] = label # Add the label (benign/malicious)\n",
        "            certificates_data.append(details)\n",
        "    return certificates_data\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 2: MODEL TRAINING AND EVALUATION (Modified from original script)\n",
        "# ==============================================================================\n",
        "\n",
        "def train_and_evaluate(data_path):\n",
        "    \"\"\"\n",
        "    Loads data, trains a Decision Tree model, evaluates it, and saves visualizations.\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting model training and evaluation process...\")\n",
        "\n",
        "    # 1. Load and preprocess the dataset\n",
        "    try:\n",
        "        data = pd.read_csv(data_path)\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"Data file not found at {data_path}. Please run the data collection part first.\")\n",
        "        return\n",
        "\n",
        "    # Convert label to numerical (0 for benign, 1 for malicious)\n",
        "    data['label'] = data['label'].apply(lambda x: 1 if x == 'malicious' else 0)\n",
        "\n",
        "    # Handle potential missing values by filling with a placeholder or median\n",
        "    data['issuer_org'].fillna('N/A', inplace=True)\n",
        "    for col in ['validity_period', 'key_length', 'san_count']:\n",
        "        data[col].fillna(data[col].median(), inplace=True)\n",
        "\n",
        "\n",
        "    X = data.drop('label', axis=1)\n",
        "    y = data['label']\n",
        "\n",
        "    # Define categorical and numerical features\n",
        "    categorical_features = ['signature_algorithm', 'issuer_org']\n",
        "    numerical_features = ['validity_period', 'key_length', 'san_count', 'self_signed']\n",
        "\n",
        "    # Create preprocessing pipelines for numerical and categorical features\n",
        "    # Numerical features will be scaled. Categorical features will be one-hot encoded.\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numerical_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features)\n",
        "        ],\n",
        "        remainder='passthrough' # Keep other columns (if any)\n",
        "    )\n",
        "\n",
        "    # 2. Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "\n",
        "    # 3. Create a full pipeline with preprocessing and the model\n",
        "    # This prevents data leakage from the test set during scaling/encoding\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', DecisionTreeClassifier(random_state=42))\n",
        "    ])\n",
        "\n",
        "    # Define hyperparameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'classifier__max_depth': [3, 5, 7, 10, None],\n",
        "        'classifier__min_samples_split': [2, 5, 10],\n",
        "        'classifier__min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "\n",
        "    # Use GridSearchCV to find the best hyperparameters\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    logging.info(f\"Best Parameters found by GridSearchCV: {grid_search.best_params_}\")\n",
        "\n",
        "    # 4. Evaluate the best model on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(\"      MODEL EVALUATION RESULTS\")\n",
        "    print(\"=\"*30)\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "    print(\"=\"*30 + \"\\n\")\n",
        "\n",
        "    # 5. Visualize Decision Tree\n",
        "    # Get feature names after one-hot encoding for the plot\n",
        "    try:\n",
        "        feature_names = (numerical_features +\n",
        "                         list(best_model.named_steps['preprocessor']\n",
        "                                       .named_transformers_['cat']\n",
        "                                       .get_feature_names_out(categorical_features)))\n",
        "\n",
        "        plt.figure(figsize=(25, 15))\n",
        "        tree.plot_tree(best_model.named_steps['classifier'],\n",
        "                      feature_names=feature_names,\n",
        "                      class_names=['Benign', 'Malicious'],\n",
        "                      filled=True,\n",
        "                      rounded=True,\n",
        "                      fontsize=10)\n",
        "        plt.title(\"Decision Tree for Malicious Certificate Detection\")\n",
        "        plt.savefig(DECISION_TREE_IMG)\n",
        "        plt.close()\n",
        "        logging.info(f\"Decision tree visualization saved to {DECISION_TREE_IMG}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Could not generate decision tree plot: {e}\")\n",
        "\n",
        "\n",
        "    # 6. Plot feature importance\n",
        "    # Extract importance from the classifier step of the pipeline\n",
        "    importances = best_model.named_steps['classifier'].feature_importances_\n",
        "\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importances\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(20)) # Show top 20 features\n",
        "    plt.title('Top 20 Feature Importances for Decision Tree Classifier')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FEATURE_IMPORTANCE_IMG)\n",
        "    plt.close()\n",
        "    logging.info(f\"Feature importance plot saved to {FEATURE_IMPORTANCE_IMG}\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN EXECUTION BLOCK\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Step 1: Collect Data ---\n",
        "    # Check if data file already exists to avoid re-collecting\n",
        "    if not os.path.exists(OUTPUT_CSV_FILE):\n",
        "        logging.info(\"Starting data collection phase...\")\n",
        "\n",
        "        if not os.path.exists(BENIGN_HOSTS_FILE) or not os.path.exists(MALICIOUS_HOSTS_FILE):\n",
        "            logging.error(\"Host files not found! Please create 'benign_hosts.txt' and 'malicious_hosts.txt'.\")\n",
        "        else:\n",
        "            benign_data = collect_data(BENIGN_HOSTS_FILE, 'benign')\n",
        "            malicious_data = collect_data(MALICIOUS_HOSTS_FILE, 'malicious')\n",
        "\n",
        "            all_data = benign_data + malicious_data\n",
        "\n",
        "            if not all_data:\n",
        "                logging.error(\"No certificate data was collected. Check your host files and network connection.\")\n",
        "            else:\n",
        "                # Convert to DataFrame and save to CSV\n",
        "                df = pd.DataFrame(all_data)\n",
        "                df.to_csv(OUTPUT_CSV_FILE, index=False)\n",
        "                logging.info(f\"Successfully collected data for {len(df)} certificates.\")\n",
        "                logging.info(f\"Dataset saved to '{OUTPUT_CSV_FILE}'. This file is preserved for forensic analysis.\")\n",
        "    else:\n",
        "        logging.info(f\"Data file '{OUTPUT_CSV_FILE}' already exists. Skipping data collection.\")\n",
        "\n",
        "    # --- Step 2: Train and Evaluate Model ---\n",
        "    if os.path.exists(OUTPUT_CSV_FILE):\n",
        "        train_and_evaluate(OUTPUT_CSV_FILE)\n",
        "    else:\n",
        "        logging.error(\"Cannot proceed to training as no data file is available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGG6T0TC7yW7",
        "outputId": "6b8a5fdd-9289-4e48-bd17-6943ef3f8687"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing benign hosts:   0%|          | 0/2000 [00:00<?, ?it/s]/tmp/ipython-input-569541297.py:61: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to not_valid_after_utc.\n",
            "  validity_period = (cert.not_valid_after - cert.not_valid_before).days\n",
            "/tmp/ipython-input-569541297.py:61: CryptographyDeprecationWarning: Properties that return a naïve datetime object have been deprecated. Please switch to not_valid_before_utc.\n",
            "  validity_period = (cert.not_valid_after - cert.not_valid_before).days\n",
            "Processing benign hosts: 100%|██████████| 2000/2000 [02:57<00:00, 11.24it/s]\n",
            "Processing malicious hosts: 100%|██████████| 2055/2055 [06:18<00:00,  5.43it/s]\n",
            "/tmp/ipython-input-569541297.py:135: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['issuer_org'].fillna('N/A', inplace=True)\n",
            "/tmp/ipython-input-569541297.py:137: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data[col].fillna(data[col].median(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "      MODEL EVALUATION RESULTS\n",
            "==============================\n",
            "Accuracy:  1.0000\n",
            "Precision: 1.0000\n",
            "Recall:    1.0000\n",
            "F1-Score:  1.0000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[600   0]\n",
            " [  0 340]]\n",
            "==============================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}